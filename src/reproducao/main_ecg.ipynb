{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# isso aqui Ã© so sair da pasta de reproducao\n",
    "if os.getcwd().split('/')[-1] == 'reproducao' or os.getcwd().split('\\\\')[-1] == 'reproducao':\n",
    "    os.chdir('..')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning.pytorch as lp\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor, TQDMProgressBar\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for cauchy multiplication not found. Install by going to extensions/cauchy/ and running `python setup.py install`. This should speed up end-to-end training by 10-50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : \n",
      "    The default C++ compiler could not be found on your system.\n",
      "    You need to either define the CXX environment variable or a symlink to the g++ command.\n",
      "    For example if g++-8 is the command you can do\n",
      "      import os\n",
      "      os.environ['CXX'] = 'g++-8'\n",
      "    \n",
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "from clinical_ts.xresnet1d import xresnet1d50,xresnet1d101\n",
    "from clinical_ts.inception1d import inception1d\n",
    "from clinical_ts.s4_model import S4Model\n",
    "from clinical_ts.misc_utils import add_default_args, LRMonitorCallback\n",
    "from reproducao_timeseries_utils import *\n",
    "from clinical_ts.schedulers import *\n",
    "from clinical_ts.eval_utils_cafa import multiclass_roc_curve\n",
    "from clinical_ts.bootstrap_utils import *\n",
    "from reproducao_mimic_ecg_preprocessing import prepare_mimic_ecg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import argparse\n",
    "\n",
    "def namespace_to_dict(namespace):\n",
    "    return {\n",
    "        k: namespace_to_dict(v) if isinstance(v, argparse.Namespace) else v\n",
    "        for k, v in vars(namespace).items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_revision_short_hash():\n",
    "    return \"\"#subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihot_encode(x, num_classes):\n",
    "    res = np.zeros(num_classes,dtype=np.float32)\n",
    "    for y in x:\n",
    "        res[y]=1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this scope to avoid pickle issues\n",
    "def mcrc_flat(targs,preds,classes):\n",
    "    _,_,res = multiclass_roc_curve(targs,preds,classes=classes)\n",
    "    return np.array(list(res.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_consistency_mapping(codes_unique, codes_unique_all, propagate_all=False):\n",
    "    res={}\n",
    "    for c in codes_unique:\n",
    "        if(propagate_all):\n",
    "            res[c]=[c[:i] for i in range(3,len(c)+1)]\n",
    "        else:#only propagate if categories are already present\n",
    "            res[c]=np.intersect1d([c[:i] for i in range(3,len(c)+1)],codes_unique_all)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main_ECG(lp.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.lr = self.hparams.lr\n",
    "\n",
    "        print(hparams)\n",
    "        if(hparams.finetune_dataset == \"thew\"):\n",
    "            num_classes = 5\n",
    "        elif(hparams.finetune_dataset == \"ribeiro_train\"):\n",
    "            num_classes = 6\n",
    "        elif(hparams.finetune_dataset == \"ptbxl_super\"):\n",
    "            num_classes = 5\n",
    "        elif(hparams.finetune_dataset == \"ptbxl_sub\"):\n",
    "            num_classes = 24\n",
    "        elif(hparams.finetune_dataset == \"ptbxl_all\"):\n",
    "            num_classes = 71\n",
    "        elif(hparams.finetune_dataset.startswith(\"segrhythm\")):\n",
    "            num_classes = int(hparams.finetune_dataset[9:])\n",
    "        elif(hparams.finetune_dataset.startswith(\"rhythm\")):\n",
    "            num_classes = int(hparams.finetune_dataset[6:])\n",
    "        elif(hparams.finetune_dataset.startswith(\"mimic\")):\n",
    "            _, lbl_itos = prepare_mimic_ecg(self.hparams.finetune_dataset,Path(self.hparams.data.split(\",\")[0]))\n",
    "            num_classes = len(lbl_itos)\n",
    "\n",
    "        # also works in the segmentation case\n",
    "        self.criterion = F.cross_entropy if (hparams.finetune_dataset == \"thew\" or hparams.finetune_dataset.startswith(\"segrhythm\"))  else F.binary_cross_entropy_with_logits\n",
    "    \n",
    "        if(hparams.architecture==\"xresnet1d50\"):\n",
    "            self.model = xresnet1d50(input_channels=hparams.input_channels, num_classes=num_classes)\n",
    "        elif(hparams.architecture==\"xresnet1d101\"):\n",
    "            self.model = xresnet1d101(input_channels=hparams.input_channels, num_classes=num_classes)\n",
    "        elif(hparams.architecture==\"inception1d\"):\n",
    "            self.model = inception1d(input_channels=hparams.input_channels, num_classes=num_classes)\n",
    "        elif(hparams.architecture==\"s4\"):\n",
    "            self.model = S4Model(d_input=hparams.input_channels, d_output=num_classes, l_max=self.hparams.input_size, d_state=self.hparams.s4_n, d_model=self.hparams.s4_h, n_layers = self.hparams.s4_layers,bidirectional=True)#,backbone=\"s4new\")\n",
    "        else:\n",
    "            assert(False)\n",
    "        \n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        # QUICK FIX FOR REMAINING NANS IN INPUT\n",
    "        x[torch.isnan(x)]=0\n",
    "        return self.model(x, **kwargs)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        for i in range(len(self.val_preds)):\n",
    "            self.on_valtest_epoch_eval({\"preds\":self.val_preds[i], \"targs\":self.val_targs[i]}, dataloader_idx=i, test=False)\n",
    "            self.val_preds[i].clear()\n",
    "            self.val_targs[i].clear()\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        for i in range(len(self.test_preds)):\n",
    "            self.on_valtest_epoch_eval({\"preds\":self.test_preds[i], \"targs\":self.test_targs[i]}, dataloader_idx=i, test=True)\n",
    "            self.test_preds[i].clear()\n",
    "            self.test_targs[i].clear()\n",
    "\n",
    "    def eval_scores(self, targs,preds,classes=None,bootstrap=False):\n",
    "        _,_,res = multiclass_roc_curve(targs,preds,classes=classes)\n",
    "        if(bootstrap):\n",
    "            point,low,high,nans = empirical_bootstrap((targs,preds), mcrc_flat, n_iterations=self.hparams.bootstrap_iterations ,score_fn_kwargs={\"classes\":classes},ignore_nans=True)\n",
    "            res2={}\n",
    "            for i,k in enumerate(res.keys()):\n",
    "                res2[k]=point[i]\n",
    "                res2[k+\"_low\"]=low[i]\n",
    "                res2[k+\"_high\"]=high[i]\n",
    "                res2[k+\"_nans\"]=nans[i]\n",
    "            return res2\n",
    "        return res \n",
    "\n",
    "    def on_valtest_epoch_eval(self, outputs_all, dataloader_idx, test=False):\n",
    "        #for dataloader_idx,outputs in enumerate(outputs_all): #multiple val dataloaders\n",
    "            preds_all = torch.cat(outputs_all[\"preds\"]).cpu()\n",
    "            targs_all = torch.cat(outputs_all[\"targs\"]).cpu()\n",
    "            # apply softmax/sigmoid to ensure that aggregated scores are calculated based on them\n",
    "            if(self.hparams.finetune_dataset == \"thew\" or self.hparams.finetune_dataset.startswith(\"segrhythm\")):\n",
    "                preds_all = F.softmax(preds_all.float(),dim=-1)\n",
    "                targs_all = torch.eye(len(self.lbl_itos))[targs_all].to(preds_all.device) \n",
    "            else:\n",
    "                preds_all = torch.sigmoid(preds_all.float())\n",
    "            \n",
    "            preds_all = preds_all.numpy()\n",
    "            targs_all = targs_all.numpy()\n",
    "            #instance level score\n",
    "            res = self.eval_scores(targs_all,preds_all,classes=self.lbl_itos,bootstrap=test)\n",
    "            res = {k+\"_auc_noagg_\"+(\"test\" if test else \"val\")+str(dataloader_idx):v for k,v in res.items()}\n",
    "            res = {k.replace(\"(\",\"_\").replace(\")\",\"_\"):v for k,v in res.items()}#avoid () for mlflow\n",
    "            self.log_dict(res)\n",
    "            print(\"epoch\",self.current_epoch,\"test\" if test else \"val\",\"noagg:\",res[\"macro_auc_noagg_\"+(\"test\" if test else \"val\")+str(dataloader_idx)])#,\"agg:\",res_agg)\n",
    "            \n",
    "            preds_all_agg,targs_all_agg = aggregate_predictions(preds_all,targs_all,self.test_idmaps[dataloader_idx] if test else self.val_idmaps[dataloader_idx],aggregate_fn=np.mean)\n",
    "            res_agg = self.eval_scores(targs_all_agg,preds_all_agg,classes=self.lbl_itos,bootstrap=test)\n",
    "            res_agg = {k+\"_auc_agg_\"+(\"test\" if test else \"val\")+str(dataloader_idx):v for k,v in res_agg.items()}\n",
    "            res_agg = {k.replace(\"(\",\"_\").replace(\")\",\"_\"):v for k,v in res_agg.items()}\n",
    "            self.log_dict(res_agg)\n",
    "\n",
    "            #export predictions\n",
    "            if(test and self.hparams.export_predictions_path!=\"\"):\n",
    "                df_test = pd.read_pickle(Path(self.hparams.export_predictions_path)/(\"df_test\"+str(dataloader_idx)+\".pkl\"))\n",
    "                df_test[\"preds\"]=list(preds_all_agg)\n",
    "                df_test[\"targs\"]=list(targs_all_agg)             \n",
    "                df_test.to_pickle(Path(self.hparams.export_predictions_path)/(\"df_test\"+str(dataloader_idx)+\".pkl\"))\n",
    "\n",
    "            print(\"epoch\",self.current_epoch,\"test\" if test else \"val\",\"agg:\",res_agg[\"macro_auc_agg_\"+(\"test\" if test else \"val\")+str(dataloader_idx)])\n",
    "            \n",
    "    def setup(self, stage):\n",
    "        rhythm = self.hparams.finetune_dataset.startswith(\"rhythm\")\n",
    "        if(rhythm):\n",
    "            num_classes_rhythm = int(self.hparams.finetune_dataset[6:])    \n",
    "\n",
    "        # configure dataset params\n",
    "        chunkify_train = self.hparams.chunkify_train\n",
    "        chunk_length_train = int(self.hparams.chunk_length_train*self.hparams.input_size) if chunkify_train else 0\n",
    "        stride_train = int(self.hparams.stride_fraction_train*self.hparams.input_size)\n",
    "        \n",
    "        chunkify_valtest = True\n",
    "        chunk_length_valtest = self.hparams.input_size if chunkify_valtest else 0\n",
    "        stride_valtest = int(self.hparams.stride_fraction_valtest*self.hparams.input_size)\n",
    "\n",
    "        train_datasets = []\n",
    "        val_datasets = []\n",
    "        test_datasets = []\n",
    "\n",
    "        self.ds_mean = None\n",
    "        self.ds_std = None\n",
    "        self.lbl_itos = None\n",
    "\n",
    "        for i,target_folder in enumerate(list(self.hparams.data.split(\",\"))):\n",
    "            \n",
    "            target_folder = Path(target_folder)           \n",
    "            \n",
    "            df_mapped, lbl_itos,  mean, std = load_dataset(target_folder)\n",
    "            # df_mapped, lbl_itos,  mean, std = load_dataset(target_folder/'processed')\n",
    "            \n",
    "            print(\"Folder:\",target_folder,\"Samples:\",len(df_mapped))\n",
    "\n",
    "            if(self.ds_mean is None):\n",
    "                if(self.hparams.finetune_dataset.startswith(\"rhythm\") or self.hparams.finetune_dataset.startswith(\"segrhythm\")):\n",
    "                    self.ds_mean = np.array([0.,0.])\n",
    "                    self.ds_std = np.array([1.,1.])\n",
    "                else:\n",
    "                    # always use PTB-XL stats\n",
    "                    self.ds_mean = np.array([-0.00184586, -0.00130277,  0.00017031, -0.00091313, -0.00148835,  -0.00174687, -0.00077071, -0.00207407,  0.00054329,  0.00155546,  -0.00114379, -0.00035649])\n",
    "                    self.ds_std = np.array([0.16401004, 0.1647168 , 0.23374124, 0.33767231, 0.33362807,  0.30583013, 0.2731171 , 0.27554379, 0.17128962, 0.14030828,   0.14606956, 0.14656108])\n",
    "\n",
    "            #specific for PTB-XL\n",
    "            if(self.hparams.finetune_dataset.startswith(\"ptbxl\")):\n",
    "                if(self.hparams.finetune_dataset==\"ptbxl_super\"):\n",
    "                    ptb_xl_label = \"label_diag_superclass\"\n",
    "                elif(self.hparams.finetune_dataset==\"ptbxl_sub\"):\n",
    "                    ptb_xl_label = \"label_diag_subclass\"\n",
    "                elif(self.hparams.finetune_dataset==\"ptbxl_all\"):\n",
    "                    ptb_xl_label = \"label_all\"\n",
    "                    \n",
    "                lbl_itos= np.array(lbl_itos[ptb_xl_label])\n",
    "                df_mapped[\"label\"]= df_mapped[ptb_xl_label+\"_filtered_numeric\"].apply(lambda x: multihot_encode(x,len(lbl_itos)))\n",
    "            elif(self.hparams.finetune_dataset == \"ribeiro_train\"):\n",
    "                df_mapped = df_mapped[df_mapped.strat_fold>=0].copy()#select on labeled subset (-1 is unlabeled)\n",
    "                df_mapped[\"label\"]= df_mapped[\"label\"].apply(lambda x: multihot_encode(x,len(lbl_itos))) #multi-hot encode\n",
    "            elif(self.hparams.finetune_dataset.startswith(\"segrhythm\")):\n",
    "                num_classes_segrhythm = int(self.hparams.finetune_dataset[9:])  \n",
    "                df_mapped = df_mapped[df_mapped.label.apply(lambda x: x<num_classes_segrhythm)]\n",
    "                lbl_itos = lbl_itos[:num_classes_segrhythm]\n",
    "                \n",
    "                \n",
    "            elif(self.hparams.finetune_dataset.startswith(\"mimic\")):\n",
    "                df_mapped, lbl_itos = prepare_mimic_ecg(self.hparams.finetune_dataset, target_folder, df_mapped=df_mapped)\n",
    "                \n",
    "            \n",
    "            if(self.lbl_itos is None):\n",
    "                self.lbl_itos = lbl_itos[:num_classes_rhythm] if rhythm else lbl_itos\n",
    "            \n",
    "            if(rhythm):\n",
    "                if(self.hparams.segmentation):\n",
    "                    tfms_ptb_xl_cpc = ToTensor(transpose_label=True)\n",
    "                else:#map to global label for given window\n",
    "                    def annotation_to_multilabel(lbl):\n",
    "                        lbl_unique = np.unique(lbl)\n",
    "                        lbl_unique = [x for x in lbl_unique if x<num_classes_rhythm]\n",
    "                        return multihot_encode(lbl_unique,num_classes_rhythm)\n",
    "                    tfms_ptb_xl_cpc = transforms.Compose([Transform(annotation_to_multilabel),ToTensor()])\n",
    "            else:\n",
    "                assert(self.hparams.segmentation is False)\n",
    "                tfms_ptb_xl_cpc = ToTensor() if self.hparams.normalize is False else transforms.Compose([Normalize(self.ds_mean,self.ds_std),ToTensor()])\n",
    "            \n",
    "            max_fold_id = df_mapped.fold.max() #unfortunately 1-based for PTB-XL; sometimes 100 (Ribeiro)\n",
    "            df_train = df_mapped[df_mapped.fold<max_fold_id-1]\n",
    "            df_val = df_mapped[df_mapped.fold==max_fold_id-1]\n",
    "            df_test = df_mapped[df_mapped.fold==max_fold_id]\n",
    "            \n",
    "            train_datasets.append(TimeseriesDatasetCrops(df_train, self.hparams.input_size, data_folder=Path(target_folder), \n",
    "                                                         chunk_length=chunk_length_train,min_chunk_length=self.hparams.input_size, stride=stride_train,transforms=tfms_ptb_xl_cpc,col_lbl =\"label\"))\n",
    "            val_datasets.append(TimeseriesDatasetCrops(df_val, self.hparams.input_size, data_folder=Path(target_folder), \n",
    "                                                       chunk_length=chunk_length_valtest,min_chunk_length=self.hparams.input_size, stride=stride_valtest,transforms=tfms_ptb_xl_cpc,col_lbl =\"label\"))\n",
    "            test_datasets.append(TimeseriesDatasetCrops(df_test, self.hparams.input_size, data_folder=Path(target_folder), \n",
    "                                                        chunk_length=chunk_length_valtest,min_chunk_length=self.hparams.input_size, stride=stride_valtest,transforms=tfms_ptb_xl_cpc,col_lbl =\"label\"))\n",
    "             \n",
    "            if(self.hparams.export_predictions_path!=\"\"):# save lbl_itos and test dataframe for later\n",
    "                np.save(Path(self.hparams.export_predictions_path)/\"lbl_itos.npy\",self.lbl_itos)\n",
    "                df_test.to_pickle(Path(self.hparams.export_predictions_path)/(\"df_test\"+str(len(test_datasets)-1)+\".pkl\"))\n",
    "\n",
    "            print(\"\\n\",target_folder)\n",
    "            if(i<len(self.hparams.data)):\n",
    "                print(\"train dataset:\",len(train_datasets[-1]),\"samples\")\n",
    "            print(\"val dataset:\",len(val_datasets[-1]),\"samples\")\n",
    "            print(\"test dataset:\",len(test_datasets[-1]),\"samples\")\n",
    "        \n",
    "\n",
    "            \n",
    "        if(len(train_datasets)>1): #multiple data folders\n",
    "            print(\"\\nCombined:\")\n",
    "            self.train_dataset = ConcatDatasetTimeseriesDatasetCrops(train_datasets)\n",
    "            self.val_datasets = [ConcatDatasetTimeseriesDatasetCrops(val_datasets)]+val_datasets\n",
    "            print(\"train dataset:\",len(self.train_dataset),\"samples\")\n",
    "            print(\"val datasets (total):\",len(self.val_datasets[0]),\"samples\")\n",
    "            self.test_datasets = [ConcatDatasetTimeseriesDatasetCrops(test_datasets)]+test_datasets\n",
    "            print(\"test datasets (total):\",len(self.test_datasets[0]),\"samples\")\n",
    "        else: #just a single data folder\n",
    "            self.train_dataset = train_datasets[0]\n",
    "            self.val_datasets = val_datasets\n",
    "            self.test_datasets = test_datasets\n",
    "\n",
    "        #create empty lists for results\n",
    "        self.val_preds=[[] for _ in range(len(self.val_datasets))]\n",
    "        self.val_targs=[[] for _ in range(len(self.val_datasets))]\n",
    "        self.test_preds=[[] for _ in range(len(self.test_datasets))]\n",
    "        self.test_targs=[[] for _ in range(len(self.test_datasets))]\n",
    "        \n",
    "        # store idmaps for aggregation\n",
    "        self.val_idmaps = [ds.get_id_mapping() for ds in self.val_datasets]\n",
    "        self.test_idmaps = [ds.get_id_mapping() for ds in self.test_datasets]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, num_workers=8, shuffle=True, drop_last = True)\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        return [DataLoader(ds, batch_size=self.hparams.batch_size, num_workers=8) for ds in self.val_datasets]\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return [DataLoader(ds, batch_size=self.hparams.batch_size, num_workers=8) for ds in self.test_datasets]\n",
    "        \n",
    "    def _step(self,data_batch, batch_idx, train, test=False, dataloader_idx=0):\n",
    "        #if(torch.sum(torch.isnan(data_batch[0])).item()>0):#debugging\n",
    "        #    print(\"nans\",torch.sum(torch.isnan(data_batch[0])).item())\n",
    "        preds_all = self.forward(data_batch[0])\n",
    "\n",
    "        loss = self.criterion(preds_all,data_batch[1])\n",
    "        self.log(\"train_loss\" if train else (\"test_loss\" if test else \"val_loss\"), loss)\n",
    "        \n",
    "        if(not train and not test):\n",
    "            self.val_preds[dataloader_idx].append(preds_all.detach())\n",
    "            self.val_targs[dataloader_idx].append(data_batch[1])\n",
    "        elif(not train and test):\n",
    "            self.test_preds[dataloader_idx].append(preds_all.detach())\n",
    "            self.test_targs[dataloader_idx].append(data_batch[1])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        return self._step(train_batch,batch_idx,train=True)\n",
    "        \n",
    "    def validation_step(self, val_batch, batch_idx, dataloader_idx=0):\n",
    "        return self._step(val_batch,batch_idx,train=False,test=False, dataloader_idx=dataloader_idx)\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx, dataloader_idx=0):\n",
    "        return self._step(test_batch,batch_idx,train=False,test=True, dataloader_idx=dataloader_idx)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        if(self.hparams.optimizer == \"sgd\"):\n",
    "            opt = torch.optim.SGD\n",
    "        elif(self.hparams.optimizer == \"adam\"):\n",
    "            opt = torch.optim.AdamW\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown Optimizer.\")\n",
    "            \n",
    "        params = self.parameters()\n",
    "\n",
    "        optimizer = opt(params, self.lr, weight_decay=self.hparams.weight_decay)\n",
    "\n",
    "        if(self.hparams.lr_schedule==\"const\"):\n",
    "            scheduler = get_constant_schedule(optimizer)\n",
    "        elif(self.hparams.lr_schedule==\"warmup-const\"):\n",
    "            scheduler = get_constant_schedule_with_warmup(optimizer,self.hparams.lr_num_warmup_steps)\n",
    "        elif(self.hparams.lr_schedule==\"warmup-cos\"):\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer,self.hparams.lr_num_warmup_steps,self.hparams.epochs*len(self.train_dataloader()),num_cycles=0.5)\n",
    "        elif(self.hparams.lr_schedule==\"warmup-cos-restart\"):\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,self.hparams.lr_num_warmup_steps,self.hparams.epochs*len(self.train_dataloader()),num_cycles=self.hparams.epochs-1)\n",
    "        elif(self.hparams.lr_schedule==\"warmup-poly\"):\n",
    "            scheduler = get_polynomial_decay_schedule_with_warmup(optimizer,self.hparams.lr_num_warmup_steps,self.hparams.epochs*len(self.train_dataloader()),num_cycles=self.hparams.epochs-1)   \n",
    "        elif(self.hparams.lr_schedule==\"warmup-invsqrt\"):\n",
    "            scheduler = get_invsqrt_decay_schedule_with_warmup(optimizer,self.hparams.lr_num_warmup_steps)\n",
    "        elif(self.hparams.lr_schedule==\"linear\"): #linear decay to be combined with warmup-invsqrt c.f. https://arxiv.org/abs/2106.04560\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, 0, self.hparams.epochs*len(self.train_dataloader()))\n",
    "        else:\n",
    "            assert(False)\n",
    "        return (\n",
    "        [optimizer],\n",
    "        [\n",
    "            {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1,\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "    def load_weights_from_checkpoint(self, checkpoint):\n",
    "        \"\"\" Function that loads the weights from a given checkpoint file. \n",
    "        based on https://github.com/PyTorchLightning/pytorch-lightning/issues/525\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage,)\n",
    "        pretrained_dict = checkpoint[\"state_dict\"]\n",
    "        model_dict = self.state_dict()\n",
    "            \n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        #S4-compatible load_state_dict\n",
    "        for name, param in self.named_parameters():\n",
    "            param.data = state_dict[name].data.to(param.device)\n",
    "        for name, param in self.named_buffers():\n",
    "            param.data = state_dict[name].data.to(param.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_checkpoint(pl_model, checkpoint_path):\n",
    "    \"\"\" load from checkpoint function that is compatible with S4\n",
    "    \"\"\"\n",
    "    lightning_state_dict = torch.load(checkpoint_path)\n",
    "    state_dict = lightning_state_dict[\"state_dict\"]\n",
    "    \n",
    "    for name, param in pl_model.named_parameters():\n",
    "        param.data = state_dict[name].data\n",
    "    for name, param in pl_model.named_buffers():\n",
    "        param.data = state_dict[name].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model_specific_args(parser):\n",
    "    parser.add_argument(\"--input-channels\", type=int, default=12)\n",
    "    parser.add_argument(\"--architecture\", type=str, help=\"xresnet1d50/xresnet1d101/inception1d/s4\", default=\"xresnet1d50\")\n",
    "    \n",
    "    parser.add_argument(\"--s4-n\", type=int, default=8, help='S4: N (Sashimi default:64)')\n",
    "    parser.add_argument(\"--s4-h\", type=int, default=512, help='S4: H (Sashimi default:64)')\n",
    "    parser.add_argument(\"--s4-layers\", type=int, default=4, help='S4: number of layers (Sashimi default:8)')\n",
    "    parser.add_argument(\"--s4-batchnorm\", action='store_true', help='S4: use BN instead of LN')\n",
    "    parser.add_argument(\"--s4-prenorm\", action='store_true', help='S4: use prenorm')\n",
    "     \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_application_specific_args(parser):\n",
    "    parser.add_argument(\"--normalize\", action='store_true', help='Normalize input using dataset stats')\n",
    "    parser.add_argument(\"--finetune-dataset\", type=str, help=\"...\", default=\"ptbxl_all\")\n",
    "    parser.add_argument(\"--chunk-length-train\", type=float, default=1.,help=\"training chunk length in multiples of input size\")\n",
    "    parser.add_argument(\"--stride-fraction-train\", type=float, default=1.,help=\"training stride in multiples of input size\")\n",
    "    parser.add_argument(\"--stride-fraction-valtest\", type=float, default=1.,help=\"val/test stride in multiples of input size\")\n",
    "    parser.add_argument(\"--chunkify-train\", action='store_true')\n",
    "    \n",
    "    parser.add_argument(\"--segmentation\", action='store_true')\n",
    "    \n",
    "    parser.add_argument(\"--eval-only\", type=str, help=\"path to model checkpoint for evaluation\", default=\"\")\n",
    "    parser.add_argument(\"--bootstrap-iterations\", type=int, help=\"number of bootstrap iterations for score estimation\", default=1000)\n",
    "\n",
    "    parser.add_argument(\"--export-predictions-path\", type=str, default=\"\", help=\"path to directory to export predictions\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = add_default_args()\n",
    "parser = add_model_specific_args(parser)\n",
    "parser = add_application_specific_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = parser.parse_args([\n",
    "    '--data', 'D:\\datasets\\MIMIC-IV-ECG-DEMO\\mimic-strodthoff', \n",
    "    '--input-size', '250', \n",
    "    '--finetune-dataset', 'mimic_all_all_allfirst_all_2000_5A', \n",
    "    '--architecture', 's4', \n",
    "    '--precision', '32', \n",
    "    '--s4-n', '8', \n",
    "    '--s4-h', '512', \n",
    "    '--batch-size', '32', \n",
    "    '--epochs', '20', \n",
    "    '--export-predictions-path', 'T(ALL2ALL)-E(ALL2ALL)/', \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '', 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.executable = \"main_ecg\"\n",
    "hparams.revision = get_git_revision_short_hash()\n",
    "if(hparams.eval_only!=\"\"):\n",
    "    hparams.epochs=0\n",
    "hparams.revision, hparams.eval_only, hparams.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.output_path = './reproducao/output'\n",
    "if not os.path.exists(hparams.output_path):\n",
    "    os.makedirs(hparams.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./reproducao/output/T(ALL2ALL)-E(ALL2ALL)/'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.export_predictions_path = f'{hparams.output_path}/{hparams.export_predictions_path}'\n",
    "if not os.path.exists(hparams.export_predictions_path):\n",
    "    os.makedirs(hparams.export_predictions_path)\n",
    "hparams.export_predictions_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(data='D:\\\\datasets\\\\MIMIC-IV-ECG-DEMO\\\\mimic-strodthoff', epochs=20, batch_size=32, lr=0.001, weight_decay=0.001, resume='', pretrained='', optimizer='adam', output_path='./reproducao/output', metadata='', gpus=1, num_nodes=1, precision=32, distributed_backend=None, accumulate=1, input_size=250, train_head_only=False, finetune=False, linear_eval=False, lr_schedule='const', lr_num_warmup_steps=1000, discriminative_lr_factor=0.1, lr_find=False, auto_batch_size=False, auc_maximization=False, refresh_rate=0, mlflow=False, input_channels=12, architecture='s4', s4_n=8, s4_h=512, s4_layers=4, s4_batchnorm=False, s4_prenorm=False, normalize=False, finetune_dataset='mimic_all_all_allfirst_all_2000_5A', chunk_length_train=1.0, stride_fraction_train=1.0, stride_fraction_valtest=1.0, chunkify_train=False, segmentation=False, eval_only='', bootstrap_iterations=1000, export_predictions_path='./reproducao/output/T(ALL2ALL)-E(ALL2ALL)/', executable='main_ecg', revision='')\n"
     ]
    }
   ],
   "source": [
    "model = Main_ECG(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./reproducao/output\\version_12\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir=hparams.output_path,\n",
    "    #version=\"\",#hparams.metadata.split(\":\")[0],\n",
    "    name=\"\")\n",
    "print(\"Output directory:\",logger.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(hparams.executable)\n",
    "mlflow.pytorch.autolog(log_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=logger.log_dir,\n",
    "    filename=\"best_model\",\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    verbose=True,\n",
    "    monitor= \"macro_auc_agg_val0\" ,#val_loss/dataloader_idx_0\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "callbacks = [checkpoint_callback,lr_monitor]#,lr_monitor2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(hparams.refresh_rate>0):\n",
    "    callbacks.append(TQDMProgressBar(refresh_rate=hparams.refresh_rate))\n",
    "hparams.refresh_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "hparams.gpus = 0\n",
    "trainer = lp.Trainer(\n",
    "    num_sanity_val_steps=0,#no debugging\n",
    "    #overfit_batches=50,#debugging\n",
    "\n",
    "    accumulate_grad_batches=hparams.accumulate,\n",
    "    max_epochs=hparams.epochs,\n",
    "    min_epochs=hparams.epochs,\n",
    "    \n",
    "    default_root_dir=hparams.output_path,\n",
    "    \n",
    "    logger=logger,\n",
    "    callbacks = callbacks,\n",
    "    benchmark=True,\n",
    "\n",
    "    accelerator=\"gpu\" if hparams.gpus>0 else \"cpu\",\n",
    "    devices=hparams.gpus if hparams.gpus>0 else 1,\n",
    "    num_nodes=hparams.num_nodes,\n",
    "    precision=hparams.precision,\n",
    "    #distributed_backend=hparams.distributed_backend,\n",
    "    \n",
    "    enable_progress_bar=hparams.refresh_rate>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(hparams.auto_batch_size):#auto tune batch size batch size\n",
    "    tuner=Tuner(trainer)\n",
    "    tuner.scale_batch_size(model, mode=\"binsearch\")\n",
    "hparams.auto_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(hparams.lr_find):# lr find\n",
    "    tuner=Tuner(trainer)\n",
    "    lr_finder = tuner.lr_find(model)\n",
    "hparams.lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: D:\\datasets\\MIMIC-IV-ECG-DEMO\\mimic-strodthoff Samples: 795\n",
      "Label set: 1076 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | S4Model | 2.7 M  | train\n",
      "------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.795    Total estimated model params size (MB)\n",
      "46        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " D:\\datasets\\MIMIC-IV-ECG-DEMO\\mimic-strodthoff\n",
      "train dataset: 419 samples\n",
      "val dataset: 24 samples\n",
      "test dataset: 32 samples\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nvrtc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(hparams\u001b[38;5;241m.\u001b[39m_get_kwargs())\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      4\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(k,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v)\u001b[38;5;66;03m#mlflow as issues with empty strings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model,ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:483\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m call_original \u001b[38;5;241m=\u001b[39m update_wrapper_extended(call_original, original)\n\u001b[0;32m    481\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_start(args, kwargs)\n\u001b[1;32m--> 483\u001b[0m patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_success(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:182\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[1;34m(original, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     result \u001b[38;5;241m=\u001b[39m patch_function(original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\pytorch\\_lightning_autolog.py:539\u001b[0m, in \u001b[0;36mpatched_fit\u001b[1;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    533\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutomatic model checkpointing is disabled because this feature only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupports pytorch-lightning >= 1.6.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         )\n\u001b[0;32m    537\u001b[0m client\u001b[38;5;241m.\u001b[39mflush(synchronous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 539\u001b[0m result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m early_stop_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    542\u001b[0m     _log_early_stop_metrics(early_stop_callback, client, run_id)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:474\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[1;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:425\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[1;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_start(og_args, og_kwargs)\n\u001b[1;32m--> 425\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m original_fn(\u001b[38;5;241m*\u001b[39mog_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mog_kwargs)\n\u001b[0;32m    427\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_success(og_args, og_kwargs)\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\mlflow\\utils\\autologging_utils\\safety.py:471\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[1;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[0;32m    468\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    469\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    470\u001b[0m ):\n\u001b[1;32m--> 471\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    595\u001b[0m     ckpt_path,\n\u001b[0;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    598\u001b[0m )\n\u001b[1;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1056\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1056\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         closure()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:176\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 176\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    179\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1273\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\optim\\adamw.py:197\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 197\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    200\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m     optimizer: Steppable,\n\u001b[0;32m    100\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    101\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[9], line 261\u001b[0m, in \u001b[0;36mMain_ECG.training_step\u001b[1;34m(self, train_batch, batch_idx)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_batch, batch_idx):\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 246\u001b[0m, in \u001b[0;36mMain_ECG._step\u001b[1;34m(self, data_batch, batch_idx, train, test, dataloader_idx)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m,data_batch, batch_idx, train, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m#if(torch.sum(torch.isnan(data_batch[0])).item()>0):#debugging\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m#    print(\"nans\",torch.sum(torch.isnan(data_batch[0])).item())\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     preds_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds_all,data_batch[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m), loss)\n",
      "Cell \u001b[1;32mIn[9], line 45\u001b[0m, in \u001b[0;36mMain_ECG.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# QUICK FIX FOR REMAINING NANS IN INPUT\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     x[torch\u001b[38;5;241m.\u001b[39misnan(x)]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\\clinical_ts\\s4_model.py:91\u001b[0m, in \u001b[0;36mS4Model.forward\u001b[1;34m(self, x, rate)\u001b[0m\n\u001b[0;32m     87\u001b[0m     z \u001b[38;5;241m=\u001b[39m norm(z\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;28;01melse\u001b[39;00m norm(z)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Apply S4 block: we ignore the state input and output\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# MODIFIED\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m z, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Dropout on the output of the S4 block\u001b[39;00m\n\u001b[0;32m     94\u001b[0m z \u001b[38;5;241m=\u001b[39m dropout(z)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\\clinical_ts\\s42.py:1126\u001b[0m, in \u001b[0;36mS4.forward\u001b[1;34m(self, u, rate, **kwargs)\u001b[0m\n\u001b[0;32m   1123\u001b[0m L \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;66;03m# Compute SS Kernel\u001b[39;00m\n\u001b[1;32m-> 1126\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (C H L) (B C H L)\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;66;03m# Convolution\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\\clinical_ts\\s42.py:1025\u001b[0m, in \u001b[0;36mHippoSSKernel.forward\u001b[1;34m(self, L, rate)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m-> 1025\u001b[0m     k, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m k\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\\clinical_ts\\s42.py:709\u001b[0m, in \u001b[0;36mSSKernelNPLR.forward\u001b[1;34m(self, state, rate, L)\u001b[0m\n\u001b[0;32m    707\u001b[0m     r \u001b[38;5;241m=\u001b[39m cauchy_mult(v, z, w, symmetric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 709\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mcauchy_conj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m r \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m*\u001b[39m dt[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# (S+1+R, C+R, H, L)\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# Low-rank Woodbury correction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\Documents\\git\\lesaude\\strodthoff2024prospects\\src\\clinical_ts\\s42.py:100\u001b[0m, in \u001b[0;36mcauchy_conj\u001b[1;34m(v, z, w)\u001b[0m\n\u001b[0;32m     97\u001b[0m z \u001b[38;5;241m=\u001b[39m _c2r(z)\n\u001b[0;32m     98\u001b[0m w \u001b[38;5;241m=\u001b[39m _c2r(w)\n\u001b[1;32m--> 100\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mcauchy_mult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _r2c(r)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\pykeops\\torch\\generic\\generic_red.py:687\u001b[0m, in \u001b[0;36mGenred.__call__\u001b[1;34m(self, backend, device_id, ranges, out, *args)\u001b[0m\n\u001b[0;32m    685\u001b[0m params\u001b[38;5;241m.\u001b[39mny \u001b[38;5;241m=\u001b[39m ny\n\u001b[0;32m    686\u001b[0m params\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m--> 687\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mGenredAutograd_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m postprocess(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_op, nout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_arg, dtype)\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\pykeops\\torch\\generic\\generic_red.py:383\u001b[0m, in \u001b[0;36mGenredAutograd_fun\u001b[1;34m(*inputs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mGenredAutograd_fun\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenredAutograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\torch\\autograd\\function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\pykeops\\torch\\generic\\generic_red.py:291\u001b[0m, in \u001b[0;36mGenredAutograd.forward\u001b[1;34m(*inputs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenredAutograd_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pichau\\miniconda3\\envs\\mimicbaseline\\lib\\site-packages\\pykeops\\torch\\generic\\generic_red.py:91\u001b[0m, in \u001b[0;36mGenredAutograd_base._forward\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m     85\u001b[0m device_id, device_args \u001b[38;5;241m=\u001b[39m set_device(\n\u001b[0;32m     86\u001b[0m     tagCPUGPU, tagHostDevice, params\u001b[38;5;241m.\u001b[39mdevice_id_request, \u001b[38;5;241m*\u001b[39margs\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpykeops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeops_io\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keops_binder\n\u001b[1;32m---> 91\u001b[0m myconv \u001b[38;5;241m=\u001b[39m \u001b[43mkeops_binder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnvrtc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtagCPUGPU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m(\n\u001b[0;32m     92\u001b[0m     tagCPUGPU,\n\u001b[0;32m     93\u001b[0m     tag1D2D,\n\u001b[0;32m     94\u001b[0m     tagHostDevice,\n\u001b[0;32m     95\u001b[0m     use_ranges,\n\u001b[0;32m     96\u001b[0m     device_id,\n\u001b[0;32m     97\u001b[0m     params\u001b[38;5;241m.\u001b[39mformula,\n\u001b[0;32m     98\u001b[0m     params\u001b[38;5;241m.\u001b[39maliases,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mlen\u001b[39m(args),\n\u001b[0;32m    100\u001b[0m     params\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    102\u001b[0m     params\u001b[38;5;241m.\u001b[39moptional_flags,\n\u001b[0;32m    103\u001b[0m )\u001b[38;5;241m.\u001b[39mimport_module()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# N.B.: KeOps C++ expects contiguous data arrays\u001b[39;00m\n\u001b[0;32m    106\u001b[0m test_contig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(arg\u001b[38;5;241m.\u001b[39mis_contiguous() \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nvrtc'"
     ]
    }
   ],
   "source": [
    "if(hparams.epochs>0 and hparams.eval_only==\"\"):\n",
    "    with mlflow.start_run(run_name=hparams.metadata) as run:\n",
    "        for k,v  in dict(hparams._get_kwargs()).items():\n",
    "            mlflow.log_param(k,\" \" if v==\"\" else v)#mlflow as issues with empty strings\n",
    "        trainer.fit(model,ckpt_path= None if hparams.resume==\"\" else hparams.resume)\n",
    "        trainer.test(model,ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label set: 1076 labels.\n"
     ]
    }
   ],
   "source": [
    "# target_folder = hparams.data.split(\",\")[0]\n",
    "# chunkify_train = hparams.chunkify_train\n",
    "# chunk_length_train = int(hparams.chunk_length_train*hparams.input_size) if chunkify_train else 0\n",
    "# stride_train = int(hparams.stride_fraction_train*hparams.input_size)\n",
    "\n",
    "# tfms_ptb_xl_cpc = ToTensor() if hparams.normalize is False else transforms.Compose([Normalize(model.ds_mean,model.ds_std),ToTensor()])\n",
    "\n",
    "# df_mapped, lbl_itos, mean, std = load_dataset(target_folder)\n",
    "# df_mapped, lbl_itos = prepare_mimic_ecg(hparams.finetune_dataset, Path(target_folder), df_mapped=df_mapped)\n",
    "# max_fold_id = df_mapped.fold.max() #unfortunately 1-based for PTB-XL; sometimes 100 (Ribeiro)\n",
    "# df_train = df_mapped[df_mapped.fold<max_fold_id-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datasets = TimeseriesDatasetCrops(df = df_train, output_size = hparams.input_size, data_folder = target_folder, \n",
    "#                                         chunk_length = chunk_length_train, min_chunk_length = hparams.input_size, \n",
    "#                                         stride = stride_train, transforms = tfms_ptb_xl_cpc, \n",
    "#                                         col_lbl = \"label\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.random.randint(low = 0, high = len(train_datasets.df_idx_mapping) - 1)\n",
    "# for idx in range(len(train_datasets.df_idx_mapping)):\n",
    "#     data, label = train_datasets.__getitem__(idx) # checking assert in getitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimicbaseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
